services:
  duckling-agent:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 7070:${HTTP_PORT}
    environment:
      - HTTP_PORT=${HTTP_PORT}
      - ENGINE_ENDPOINT=${DMR_BASE_URL}/engines/llama.cpp/v1
      - DMR_CHAT_MODEL=${DMR_CHAT_MODEL}
      - SYSTEM_INSTRUCTION=${SYSTEM_INSTRUCTION}
    depends_on:
      - download-chat-model

  download-chat-model:
    provider:
      type: model
      options:
        model: ${DMR_CHAT_MODEL}
