services:
  tiny-agent:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 7070:${HTTP_PORT}
    environment:
      - HTTP_PORT=${HTTP_PORT}
      - ENGINE_ENDPOINT=${DMR_BASE_URL}/engines/llama.cpp/v1
      - DMR_CHAT_MODEL=${DMR_CHAT_MODEL}
      - SYSTEM_INSTRUCTION= |
          You are a helpful assistant. You are a Star Trek expert.
          Answer the user's questions to the best of your ability.
          If you don't know the answer, say "I don't know" instead of making up an answer.
    depends_on:
      - download-chat-model

  download-chat-model:
    provider:
      type: model
      options:
        model: ${DMR_CHAT_MODEL}
